import Mathlib.Analysis.NormedSpace.HahnBanach.Extension
import Mathlib.Analysis.Normed.Module.WeakDual
import Mathlib.Analysis.Normed.Group.Quotient
import Mathlib.Analysis.SpecialFunctions.Trigonometric.Deriv
import Mathlib.Analysis.Normed.Module.FiniteDimension
import Mathlib.Analysis.Calculus.Deriv.Prod
import Mathlib.Analysis.Calculus.Implicit
import Mathlib.Analysis.Calculus.InverseFunctionTheorem.FDeriv
import Mathlib.Analysis.Calculus.InverseFunctionTheorem.FDeriv
import Mathlib.Topology.MetricSpace.Contracting

open BigOperators Function Set Real Topology Filter
open Interval ENNReal
noncomputable section

/- # Last time

Last time we discussed topology:
* `TopologicalSpace X` states that `X` is a topological space.
* continuity via `Filter.Tendsto` and via open sets
* homeomorphisms; compactness; connectedness
* various separation axioms
* the `fun_prop` tactics for proving goals of the form
  "the composition of continuous functions is continuous"
* `MetricSpace X` states that `X` is a metric space.


Today: differential calculus and differentiation
-/




/- # Differential Calculus -/

/- We write `deriv` to compute the derivative of a function.
`simp` can compute the derivatives of standard functions. -/

example (x : ‚Ñù) : deriv Real.sin x = Real.cos x := by sorry

example (x : ‚ÑÇ) :
    deriv (fun y ‚Ü¶ Complex.sin (y + 3)) x = Complex.cos (x + 3) := by sorry

/- Not every function has a derivative.
As usual, in Mathlib we just define the derivative
of a non-differentiable function to be `0`. -/

variable (f : ‚Ñù ‚Üí ‚Ñù) (x : ‚Ñù) in
#check (deriv_zero_of_not_differentiableAt :
  ¬¨ DifferentiableAt ‚Ñù f x ‚Üí deriv f x = 0)

/- So proving that `deriv f x = y` doesn't
necessarily mean that `f` is differentiable.
Often it is nicer to use the predicate `HasDerivAt f y x`,
which states that `f` is differentiable and `f'(x) = y`. -/

example (x : ‚Ñù) : HasDerivAt Real.sin (Real.cos x) x :=
  sorry


/- We can also specify that a function has a derivative
without specifying its derivative. -/

example (x : ‚Ñù) : DifferentiableAt ‚Ñù sin x :=
  sorry


/- Mathlib contains lemmas stating that common operations satisfy
`HasDerivAt` and `DifferentiableAt` and to compute `deriv`. -/

#check HasDerivAt.add
#check deriv_add
#check DifferentiableAt.add


example (x : ‚Ñù) :
    HasDerivAt (fun x ‚Ü¶ Real.cos x + Real.sin x)
    (Real.cos x - Real.sin x) x := by
  -- rw [sub_eq_neg_add]
  sorry
  done


/- There are various variations of derivatives/being differentiable -/

/- A function is differentiable everywhere. -/
#check Differentiable

/- A function is differentiable on a subset. -/
#check DifferentiableOn

/- A function is differentiable at a point, considered only within the subset -/
#check DifferentiableWithinAt

/- We can also consider the derivative only within a subset. -/
#check HasDerivWithinAt
#check derivWithin




/-
Recall Lean's notation for intervals: `Icc a b = [a, b]` is a closed interval

The **intermediate value theorem** states that if `f` is continuous and
`f a ‚â§ y ‚â§ f b`, then there is an `x ‚àà [a, b]` with `f(x) = y`.
-/

example {f : ‚Ñù ‚Üí ‚Ñù} {a b : ‚Ñù} (hab : a ‚â§ b)
    (hf : ContinuousOn f (Icc a b)) :
    Icc (f a) (f b) ‚äÜ f '' Icc a b :=
  sorry

/- The deeper mathematical reason is that intervals are connected (and vice versa).
Continuous functions preserve connectedness.
-/
#check isConnected_Icc
#check IsPreconnected.mem_intervals

#check IsConnected.image

#check IsConnected.Icc_subset

-- Let's put this together ourselves.
example {f : ‚Ñù ‚Üí ‚Ñù} {a b : ‚Ñù} (hab : a ‚â§ b)
    (hf : ContinuousOn f (Icc a b)) :
    Icc (f a) (f b) ‚äÜ f '' Icc a b := by
  sorry








/-
The mean value theorem states that if `f` is continous on `[a, b]`
and differentiable on `(a, b)` then there is a `c ‚àà (a, b)` where `f'(c)` is the
average slope of `f` on `[a, b]`
-/
example (f : ‚Ñù ‚Üí ‚Ñù) {a b : ‚Ñù} (hab : a < b)
    (hf : ContinuousOn f (Icc a b))
    (hf' : DifferentiableOn ‚Ñù f (Ioo a b)) :
    ‚àÉ c ‚àà Ioo a b, deriv f c = (f b - f a) / (b - a) :=
  sorry


/- Rolle's theorem is the special case where `f a = f b`.
Why is there no differentiability requirement on `f` here? -/
example {f : ‚Ñù ‚Üí ‚Ñù} {a b : ‚Ñù} (hab : a < b)
    (hfc : ContinuousOn f (Icc a b)) (hfI : f a = f b) :
    ‚àÉ c ‚àà Ioo a b, deriv f c = 0 :=
  sorry



/- We can more generally talk about the derivative of functions between normed spaces.

A *normed group* is an abelian group with a norm satisfying the following rules.
-/

section NormedGroup

variable {E : Type*} [NormedAddCommGroup E]

#check (fun x ‚Ü¶ ‚Äñx‚Äñ : E ‚Üí ‚Ñù)

example (x : E) : 0 ‚â§ ‚Äñx‚Äñ :=
  sorry

example {x : E} : ‚Äñx‚Äñ = 0 ‚Üî x = 0 :=
  sorry

example (x y : E) : ‚Äñx + y‚Äñ ‚â§ ‚Äñx‚Äñ + ‚Äñy‚Äñ :=
  sorry

/- This turns `E` into a metric space. -/
example (x y : E) : dist x y = ‚Äñx - y‚Äñ :=
  sorry

/- A *normed space* is a normed group that is a vector space
satisfying the following condition. -/

variable [NormedSpace ‚Ñù E]

example (a : ‚Ñù) (x : E) : ‚Äña ‚Ä¢ x‚Äñ = |a| * ‚Äñx‚Äñ :=
  sorry


/- A complete normed space is known as a *Banach space*.
Every finite-dimensional vector space is complete. -/

example [FiniteDimensional ‚Ñù E] : CompleteSpace E := by infer_instance

-- Products of Banach spaces are also Banach.
example [CompleteSpace E] : CompleteSpace (E √ó E) := by infer_instance

-- A quotient of a Banach space is also Banach.
example [CompleteSpace E] (s : Submodule ‚Ñù E) : CompleteSpace (E ‚ß∏ s) := by infer_instance

-- The Banach fixed point theorem
#check ContractingWith.exists_fixedPoint

/- In the above examples, we could also replace `‚Ñù` by `‚ÑÇ`
or another *normed field*. -/
#check NormedField

#check TopologicalSpace.SeparableSpace

-- The continuous dual space: all continuous linear maps `E ‚ÜíL[R] R`
#check StrongDual

#check Module.IsReflexive

-- Hahn-Banach theorem
#check exists_extension_norm_eq

-- Banach-Alaoglu theorem
#check WeakDual.isCompact_polar

-- Mathlib also has the closed graph theorem.

end NormedGroup



/- We can also take the derivative of functions that take values in a
normed vector space. -/

-- Proving differentiability is easy using `fun_prop`.
example {x : ‚Ñù} : DifferentiableAt ‚Ñù (fun x ‚Ü¶ ((Real.cos x) ^ 2, (Real.sin x) ^ 2)) x := by
  fun_prop

-- Currently, derivatives still have to be computed by hand.
example (x : ‚Ñù) : deriv (fun x ‚Ü¶ ((Real.cos x) ^ 2, (Real.sin x) ^ 2)) x =
    (- 2 * Real.cos x * Real.sin x, 2 * Real.sin x * Real.cos x) := by
  sorry







  -- suffices HasDerivAt (fun x ‚Ü¶ cos x ^ 2) (2 * (cos x) ^ 1 * (- sin x)) x by
  done



/- If the domain is a normed space we can define the
total derivative, which will be a continuous linear map. -/

/- Morphisms between normed spaces are continuous linear maps `E ‚ÜíL[ùïú] F`. -/
section NormedSpace

variable {ùïú : Type*} [NontriviallyNormedField ùïú]
  {E : Type*} [NormedAddCommGroup E] [NormedSpace ùïú E]
  {F : Type*} [NormedAddCommGroup F] [NormedSpace ùïú F]
  {G : Type*} [NormedAddCommGroup G] [NormedSpace ùïú G]


example : E ‚ÜíL[ùïú] E := ContinuousLinearMap.id ùïú E

example (f : E ‚ÜíL[ùïú] F) : E ‚Üí F := f

example (f : E ‚ÜíL[ùïú] F) : Continuous f := f.cont

example (f : E ‚ÜíL[ùïú] F) : E ‚Üí‚Çó[ùïú] F := f

example (f : E ‚ÜíL[ùïú] F) (g : F ‚ÜíL[ùïú] G) : E ‚ÜíL[ùïú] G := g.comp f

/- Isomorphisms between normed spaces are continuous linear equivalences `E ‚âÉL[ùïú] F` -/
example : E ‚âÉL[ùïú] E := ContinuousLinearEquiv.refl ùïú E

-- We can invert and compose continuous linear equivalences: these operations are called
-- `symm` and `trans`, like for linear equivalences.
example (f : E ‚âÉL[ùïú] F) : F ‚âÉL[ùïú] E := f.symm

example (f : E ‚âÉL[ùïú] F) (g : F ‚âÉL[ùïú] G) : E ‚âÉL[ùïú] G := f.trans g



/- Continuous linear maps have an operator norm. -/

example (f : E ‚ÜíL[ùïú] F) (x : E) : ‚Äñf x‚Äñ ‚â§ ‚Äñf‚Äñ * ‚Äñx‚Äñ :=
  sorry

example (f : E ‚ÜíL[ùïú] F) {M : ‚Ñù} (hMp : 0 ‚â§ M)
    (hM : ‚àÄ x, ‚Äñf x‚Äñ ‚â§ M * ‚Äñx‚Äñ) : ‚Äñf‚Äñ ‚â§ M :=
  sorry


/- We define the *Fr√©chet derivative* of any function between normed spaces. -/

example (f : E ‚Üí F) (f' : E ‚ÜíL[ùïú] F) (x‚ÇÄ : E) :
    HasFDerivAt f f' x‚ÇÄ ‚Üî
    Tendsto (fun x ‚Ü¶ ‚Äñf x - f x‚ÇÄ - f' (x - x‚ÇÄ)‚Äñ / ‚Äñx - x‚ÇÄ‚Äñ) (ùìù x‚ÇÄ) (ùìù 0) := by
  sorry -- let's find a good lemma using rw??

example (f : E ‚Üí F) (f' : E ‚ÜíL[ùïú] F) (x‚ÇÄ : E) (hff' : HasFDerivAt f f' x‚ÇÄ) :
    fderiv ùïú f x‚ÇÄ = f' :=
  sorry

-- Like for the derivative, we also have a Fr√©chet derivative within a set.
variable (f : E ‚Üí F) (f' : E ‚ÜíL[ùïú] F) (s : Set E) (x‚ÇÄ : E) in
#check fderivWithin ùïú f s x‚ÇÄ

#check HasFDerivWithinAt.fderivWithin


-- Careful: in higher dimensions, a function can have several derivatives within a set,
-- if that set is sufficiently "bad".
-- However, on "nice" sets, it is: this includes open sets and convex sets with non-empty interior.
#check UniqueDiffOn

example {s : Set E} (hs : IsOpen s) : UniqueDiffOn ùïú s := by sorry

#check Convex
#check uniqueDiffOn_convex





/- We can take the directional derivative or partial derivative
by applying the Fr√©chet derivative to an argument -/
example (x y : ‚Ñù) :
    let f := fun ((x,y) : ‚Ñù √ó ‚Ñù) ‚Ü¶ x^2 + x * y
    fderiv ‚Ñù f (x, y) (1, 0) = 2 * x + y := by
  sorry -- exercise


/- We write `ContDiff ùïú n f` to say that `f` is `C^n`,
i.e. it is `n`-times continuously differentiable.
Here `n` lives in `WithTop ‚Ñï‚àû`:
`‚Ñï‚àû` is `‚Ñï` with an extra top element `‚àû` added ("‚àû"),
and `WithTop ‚Ñï‚àû` adds another element `‚ä§` ("œâ").
-/
variable {f g : E ‚Üí F} {m : ‚Ñï‚àû} {r : ùïú}

open scoped ContDiff -- for the notation "‚àû"

#check ContDiff ùïú 42 f
#check ContDiff ùïú ‚àû f -- f is smooth



example : ContDiff ùïú 0 f ‚Üî Continuous f := contDiff_zero

example {n : ‚Ñï} : ContDiff ùïú (n+1) f ‚Üî
    Differentiable ùïú f ‚àß ContDiff ùïú n (fderiv ùïú f) := by
  sorry


example : ContDiff ùïú ‚àû f ‚Üî ‚àÄ n : ‚Ñï, ContDiff ùïú n f :=
  sorry



/- The element œâ denotes analytic functions: those which have a Taylor series which converges
to the function -/
#check ContDiff ùïú œâ f -- f is analytic

#check AnalyticAt

example [CompleteSpace F] : ContDiff ùïú œâ f ‚Üî ‚àÄ x, AnalyticAt ùïú f x := sorry

/- `fun_prop` can also prove that simple functions are `C^n`,
and knows about the relation between differentiability and `C^n` functions -/
variable {f g : E ‚Üí F} {n : ‚Ñï‚àû} {r : ùïú}
example (hf : ContDiff ùïú n f) (hg : ContDiff ùïú n g) :
    ContDiff ùïú n (fun x ‚Ü¶ (f x, r ‚Ä¢ f x + g x)) := by
  sorry

-- The implicit function theorem
#check ImplicitFunctionData.implicitFunction

-- The inverse function theorem
#check HasStrictFDerivAt.to_localInverse


/- If `f` is C¬π, its `fderiv` is continuous -/
#check ContDiff.continuous_fderiv

-- There is also a converse: if `f` is differentiable and has continuous fderiv, it is C¬π.
#check contDiff_one_iff_fderiv

-- This also holds for higher smoothness.
#check contDiff_succ_iff_fderiv


-- We can also take higher derivatives: these are denoted `iteratedFDeriv`
#check iteratedFDeriv

-- If `f : E ‚Üí F`, then `fderiv ùïú f x : E ‚ÜíL[ùïú] F` is a continuous linear map.
#check fderiv ùïú f

-- The `fderiv` of that is a continuous bilinear map, and so on:
variable {x‚ÇÄ : E}
#check iteratedFDeriv ùïú 2 f x‚ÇÄ

#check ContinuousMultilinearMap

-- The 0-th iterated fderiv is the function itself.
#check iteratedFDeriv_zero_apply

-- The first iterated fderiv is the function itself.
-- Strictly speaking, their types are slightly different.
example (m : Fin 1 ‚Üí E) : iteratedFDeriv ùïú 1 f x‚ÇÄ m = (fderiv ùïú f x‚ÇÄ) (m 0) := by
  sorry

-- The k+1-th iterated fderiv is the fderiv of the k-th one.
#check iteratedFDeriv_succ_apply_left

end NormedSpace
