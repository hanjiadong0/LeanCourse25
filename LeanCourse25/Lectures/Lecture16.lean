import Mathlib.Analysis.NormedSpace.HahnBanach.Extension
import Mathlib.Analysis.Normed.Module.WeakDual
import Mathlib.Analysis.Normed.Group.Quotient
import Mathlib.Analysis.SpecialFunctions.Trigonometric.Deriv
import Mathlib.Analysis.Normed.Module.FiniteDimension
import Mathlib.Analysis.Calculus.Deriv.Prod
import Mathlib.Analysis.Calculus.Implicit
import Mathlib.Analysis.Calculus.InverseFunctionTheorem.FDeriv
import Mathlib.Analysis.Calculus.InverseFunctionTheorem.FDeriv
import Mathlib.Topology.MetricSpace.Contracting

open BigOperators Function Set Real Topology Filter
open Interval ENNReal
noncomputable section

/- # Last time

Last time we discussed topology:
* `TopologicalSpace X` states that `X` is a topological space.
* continuity via `Filter.Tendsto` and via open sets
* homeomorphisms; compactness; connectedness
* various separation axioms
* the `fun_prop` tactics for proving goals of the form
  "the composition of continuous functions is continuous"
* `MetricSpace X` states that `X` is a metric space.


Today: differential calculus and differentiation
-/




/- # Differential Calculus -/

/- We write `deriv` to compute the derivative of a function.
`simp` can compute the derivatives of standard functions. -/

example (x : ‚Ñù) : deriv Real.sin x = Real.cos x := by simp

example (x : ‚ÑÇ) :
    deriv (fun y ‚Ü¶ Complex.sin (y + 3)) x = Complex.cos (x + 3) := by simp

/- Not every function has a derivative.
As usual, in Mathlib we just define the derivative
of a non-differentiable function to be `0`. -/

variable (f : ‚Ñù ‚Üí ‚Ñù) (x : ‚Ñù) in
#check (deriv_zero_of_not_differentiableAt :
  ¬¨ DifferentiableAt ‚Ñù f x ‚Üí deriv f x = 0)

/- So proving that `deriv f x = y` doesn't
necessarily mean that `f` is differentiable.
Often it is nicer to use the predicate `HasDerivAt f y x`,
which states that `f` is differentiable and `f'(x) = y`. -/

example (x : ‚Ñù) : HasDerivAt Real.sin (Real.cos x) x :=
  hasDerivAt_sin x


/- We can also specify that a function has a derivative
without specifying its derivative. -/

example (x : ‚Ñù) : DifferentiableAt ‚Ñù sin x :=
  differentiableAt_sin

-- Note: the argument `‚Ñù` is the field over which we are working,
-- not the domain of the sin function.
-- For instance, this is how to say "the Complex sin function is real differentiable".
example (z : ‚ÑÇ) : DifferentiableAt ‚Ñù Complex.sin z := sorry


#check HasDerivAt.differentiableAt

/- Mathlib contains lemmas stating that common operations satisfy
`HasDerivAt` and `DifferentiableAt` and to compute `deriv`. -/

#check HasDerivAt.add
#check deriv_add
#check DifferentiableAt.add


example (x : ‚Ñù) :
    HasDerivAt (fun x ‚Ü¶ Real.cos x + Real.sin x)
    (Real.cos x - Real.sin x) x := by
  rw [sub_eq_neg_add]
  apply HasDerivAt.add
  ¬∑ exact hasDerivAt_cos x
  ¬∑ exact hasDerivAt_sin x
  done


/- There are various variations of derivatives/being differentiable -/

/- A function is differentiable everywhere. -/
#check Differentiable

/- A function is differentiable on a subset. -/
#check DifferentiableOn

/- A function is differentiable at a point, considered only within the subset -/
#check DifferentiableWithinAt

/- We can also consider the derivative only within a subset. -/
#check HasDerivWithinAt
#check derivWithin




/-
Recall Lean's notation for intervals: `Icc a b = [a, b]` is a closed interval

The **intermediate value theorem** states that if `f` is continuous and
`f a ‚â§ y ‚â§ f b`, then there is an `x ‚àà [a, b]` with `f(x) = y`.
-/

example {f : ‚Ñù ‚Üí ‚Ñù} {a b : ‚Ñù} (hab : a ‚â§ b)
    (hf : ContinuousOn f (Icc a b)) :
    Icc (f a) (f b) ‚äÜ f '' Icc a b :=
  intermediate_value_Icc hab hf

-- There is also a version for `f a ‚â• y ‚â• f b`, and one for unordered intervals.
-- `Set.uIcc a b` denotes the *unordered* closed interval `[[a, b]]`:
-- depending on whether `a ‚â§ b` or `b ‚â§ a`,
-- it is either `Icc a b` or `Icc b a`.
#check Set.uIcc


/- The deeper mathematical reason is that intervals are connected (and vice versa).
Continuous functions preserve connectedness.
-/
#check isConnected_Icc
#check IsPreconnected.mem_intervals

#check IsConnected.image

#check IsConnected.Icc_subset

-- Let's put this together ourselves.
example {f : ‚Ñù ‚Üí ‚Ñù} {a b : ‚Ñù} (hab : a ‚â§ b)
    (hf : ContinuousOn f (Icc a b)) :
    Icc (f a) (f b) ‚äÜ f '' Icc a b := by
  have : IsConnected (Icc a b) := isConnected_Icc hab
  have : IsConnected (f '' Icc a b) := this.image f hf
  apply this.Icc_subset
  ¬∑ refine mem_image_of_mem f ?_
    exact left_mem_Icc.mpr hab
  ¬∑ apply mem_image_of_mem
    exact right_mem_Icc.mpr hab









/-
The mean value theorem states that if `f` is continous on `[a, b]`
and differentiable on `(a, b)` then there is a `c ‚àà (a, b)` where `f'(c)` is the
average slope of `f` on `[a, b]`
-/
example (f : ‚Ñù ‚Üí ‚Ñù) {a b : ‚Ñù} (hab : a < b)
    (hf : ContinuousOn f (Icc a b))
    (hf' : DifferentiableOn ‚Ñù f (Ioo a b)) :
    ‚àÉ c ‚àà Ioo a b, deriv f c = (f b - f a) / (b - a) :=
  exists_deriv_eq_slope f hab hf hf'


/- Rolle's theorem is the special case where `f a = f b`.
Why is there no differentiability requirement on `f` here? -/
example {f : ‚Ñù ‚Üí ‚Ñù} {a b : ‚Ñù} (hab : a < b)
    (hfc : ContinuousOn f (Icc a b)) (hfI : f a = f b) :
    ‚àÉ c ‚àà Ioo a b, deriv f c = 0 :=
  exists_deriv_eq_zero hab hfc hfI



/- We can more generally talk about the derivative of functions between normed spaces.

A *normed group* is an abelian group with a norm satisfying the following rules.
-/

section NormedGroup

variable {E : Type*} [NormedAddCommGroup E]

#check (fun x ‚Ü¶ ‚Äñx‚Äñ : E ‚Üí ‚Ñù)

example (x : E) : 0 ‚â§ ‚Äñx‚Äñ :=
  norm_nonneg x

example {x : E} : ‚Äñx‚Äñ = 0 ‚Üî x = 0 :=
  norm_eq_zero

example (x y : E) : ‚Äñx + y‚Äñ ‚â§ ‚Äñx‚Äñ + ‚Äñy‚Äñ :=
  norm_add_le x y

/- This turns `E` into a metric space. -/
example : MetricSpace E := by infer_instance
/- The metric is induced by the norm. -/
example (x y : E) : dist x y = ‚Äñx - y‚Äñ := by
  exact NormedAddGroup.dist_eq x y



/- A *normed space* is a normed group that is a vector space
satisfying the following condition. -/

variable [NormedSpace ‚Ñù E]

example (a : ‚Ñù) (x : E) : ‚Äña ‚Ä¢ x‚Äñ = |a| * ‚Äñx‚Äñ := by
  rw [norm_smul a x]
  rw [norm_eq_abs]


/- A complete normed space is known as a *Banach space*.
Every finite-dimensional vector space is complete. -/

example [FiniteDimensional ‚Ñù E] : CompleteSpace E := by infer_instance

-- Products of Banach spaces are also Banach.
example [CompleteSpace E] : CompleteSpace (E √ó E) := by infer_instance

-- A quotient of a Banach space is also Banach.
example [CompleteSpace E] (s : Submodule ‚Ñù E) : CompleteSpace (E ‚ß∏ s) := by infer_instance

-- The Banach fixed point theorem
#check ContractingWith.exists_fixedPoint

/- In the above examples, we could also replace `‚Ñù` by `‚ÑÇ`
or another *normed field*. -/
#check NormedField


#check TopologicalSpace.SeparableSpace

-- The continuous dual space: all continuous linear maps `E ‚ÜíL[R] R`
#check StrongDual

#check Module.IsReflexive

-- Hahn-Banach theorem
#check exists_extension_norm_eq

-- Banach-Alaoglu theorem
#check WeakDual.isCompact_polar

-- Mathlib also has the closed graph theorem.

end NormedGroup



/- We can also take the derivative of functions that take values in a
normed vector space. -/

/- This is why we need to specify the base field in `DifferentiableAt`:
for instance, a complex normed space is also a real normed space,
and being complex differentiable is stronger than being real differentiable. -/


-- Proving differentiability is easy using `fun_prop`.
example {x : ‚Ñù} : DifferentiableAt ‚Ñù (fun x ‚Ü¶ ((Real.cos x) ^ 2, (Real.sin x) ^ 2)) x := by
  fun_prop

-- Currently, derivatives still have to be computed by hand.
example (x : ‚Ñù) : deriv (fun x ‚Ü¶ ((Real.cos x) ^ 2, (Real.sin x) ^ 2)) x =
    (- 2 * Real.cos x * Real.sin x, 2 * Real.sin x * Real.cos x) := by
  apply HasDerivAt.deriv
  refine HasDerivAt.prodMk ?_ ?_ -- apply?
  ¬∑ -- Careful: the `suffices` tactic has syntax involving `by`, but **not** `:= by`!
    suffices HasDerivAt (fun x ‚Ü¶ cos x ^ 2) (2 * (cos x) ^ 1 * (- sin x)) x by
      -- Other proofs would be `simp_all` or `simpa`.
      -- `simp_all` simplifies both the goal and all local hypotheses, using all local hypotheses.
      -- `simpa` is shorthand for `simp; assumption`. You can also provide an explicit
      -- term to prove the goal: writing `simpa using h` runs `simp` on the goal and on `h`.
      -- If the simplified `h` does not prove the goal, it fails.
      field_simp at this ‚ä¢
      exact this
    apply HasDerivAt.pow
    exact hasDerivAt_cos x
  ¬∑ -- The `convert` tactic is similar to `refine`: it tries to match the term
    -- provided to it with the goal and creates a new goal for each goal
    -- that you need to prove, and each part of the term that does not match exactly.
    -- In this case, there are two goals, one about unifying `sin x ^ {2 - 1}` and `sin`,
    -- and another about `HasDerivAt` and `sin`.
    convert HasDerivAt.pow ?_ 2
    ¬∑ simp
    ¬∑ exact hasDerivAt_sin x
  done



/- If the domain is a normed space we can define the
total derivative, which will be a continuous linear map. -/

/- Morphisms between normed spaces are continuous linear maps `E ‚ÜíL[ùïú] F`. -/
section NormedSpace

variable {ùïú : Type*} [NontriviallyNormedField ùïú]
  {E : Type*} [NormedAddCommGroup E] [NormedSpace ùïú E]
  {F : Type*} [NormedAddCommGroup F] [NormedSpace ùïú F]
  {G : Type*} [NormedAddCommGroup G] [NormedSpace ùïú G]


example : E ‚ÜíL[ùïú] E := ContinuousLinearMap.id ùïú E

example (f : E ‚ÜíL[ùïú] F) : E ‚Üí F := f

example (f : E ‚ÜíL[ùïú] F) : Continuous f := f.cont

example (f : E ‚ÜíL[ùïú] F) : E ‚Üí‚Çó[ùïú] F := f

example (f : E ‚ÜíL[ùïú] F) (g : F ‚ÜíL[ùïú] G) : E ‚ÜíL[ùïú] G := g.comp f

/- Isomorphisms between normed spaces are continuous linear equivalences `E ‚âÉL[ùïú] F` -/
example : E ‚âÉL[ùïú] E := ContinuousLinearEquiv.refl ùïú E

-- We can invert and compose continuous linear equivalences: these operations are called
-- `symm` and `trans`, like for linear equivalences.
example (f : E ‚âÉL[ùïú] F) : F ‚âÉL[ùïú] E := f.symm

example (f : E ‚âÉL[ùïú] F) (g : F ‚âÉL[ùïú] G) : E ‚âÉL[ùïú] G := f.trans g



/- Continuous linear maps have an operator norm. -/

example (f : E ‚ÜíL[ùïú] F) (x : E) : ‚Äñf x‚Äñ ‚â§ ‚Äñf‚Äñ * ‚Äñx‚Äñ :=
  ContinuousLinearMap.le_opNorm f x

example (f : E ‚ÜíL[ùïú] F) {M : ‚Ñù} (hMp : 0 ‚â§ M)
    (hM : ‚àÄ x, ‚Äñf x‚Äñ ‚â§ M * ‚Äñx‚Äñ) : ‚Äñf‚Äñ ‚â§ M :=
  (ContinuousLinearMap.opNorm_le_iff hMp).mpr hM


/- We define the *Fr√©chet derivative* of any function between normed spaces. -/

example (f : E ‚Üí F) (f' : E ‚ÜíL[ùïú] F) (x‚ÇÄ : E) :
    HasFDerivAt f f' x‚ÇÄ ‚Üî
    Tendsto (fun x ‚Ü¶ ‚Äñf x - f x‚ÇÄ - f' (x - x‚ÇÄ)‚Äñ / ‚Äñx - x‚ÇÄ‚Äñ) (ùìù x‚ÇÄ) (ùìù 0) := by
  rw [hasFDerivAt_iff_tendsto]
  field_simp
  -- or: simp_rw [div_eq_inv_mul, hasFDerivAt_iff_tendsto]

example (f : E ‚Üí F) (f' : E ‚ÜíL[ùïú] F) (x‚ÇÄ : E) (hff' : HasFDerivAt f f' x‚ÇÄ) :
    fderiv ùïú f x‚ÇÄ = f' :=
  HasFDerivAt.fderiv hff'

-- Like for the derivative, we also have a Fr√©chet derivative within a set.
variable (f : E ‚Üí F) (f' : E ‚ÜíL[ùïú] F) (s : Set E) (x‚ÇÄ : E) in
#check fderivWithin ùïú f s x‚ÇÄ

#check HasFDerivWithinAt.fderivWithin


-- Careful: in higher dimensions, a function can have several derivatives within a set,
-- if that set is sufficiently "bad".
-- However, on "nice" sets, it is: this includes open sets and convex sets with non-empty interior.
#check UniqueDiffOn

example {s : Set E} (hs : IsOpen s) : UniqueDiffOn ùïú s := hs.uniqueDiffOn

#check Convex
#check uniqueDiffOn_convex





/- We can take the directional derivative or partial derivative
by applying the Fr√©chet derivative to an argument -/
example (x y : ‚Ñù) :
    let f := fun ((x,y) : ‚Ñù √ó ‚Ñù) ‚Ü¶ x^2 + x * y
    fderiv ‚Ñù f (x, y) (1, 0) = 2 * x + y := by
  sorry -- exercise


/- We write `ContDiff ùïú n f` to say that `f` is `C^n`,
i.e. it is `n`-times continuously differentiable.
Here `n` lives in `WithTop ‚Ñï‚àû`:
`‚Ñï‚àû` is `‚Ñï` with an extra top element `‚àû` added ("‚àû"),
and `WithTop ‚Ñï‚àû` adds another element `‚ä§` ("œâ").
-/
variable {f g : E ‚Üí F} {m : ‚Ñï‚àû} {r : ùïú}

open scoped ContDiff -- for the notation "‚àû"

#check ContDiff ùïú 37 f
#check ContDiff ùïú ‚àû f -- f is smooth



example : ContDiff ùïú 0 f ‚Üî Continuous f := contDiff_zero

example {n : ‚Ñï} : ContDiff ùïú (n+1) f ‚Üî
    Differentiable ùïú f ‚àß ContDiff ùïú n (fderiv ùïú f) := by
  simp [contDiff_succ_iff_fderiv]


example : ContDiff ùïú ‚àû f ‚Üî ‚àÄ n : ‚Ñï, ContDiff ùïú n f :=
  contDiff_infty



/- The element œâ denotes analytic functions: those which have a Taylor series which converges
to the function -/
#check ContDiff ùïú œâ f -- f is analytic

#check AnalyticAt

example [CompleteSpace F] : ContDiff ùïú œâ f ‚Üî ‚àÄ x, AnalyticAt ùïú f x := sorry

/- `fun_prop` can also prove that simple functions are `C^n`,
and knows about the relation between differentiability and `C^n` functions -/
variable {f g : E ‚Üí F} {n : ‚Ñï‚àû} {r : ùïú}
example (hf : ContDiff ùïú n f) (hg : ContDiff ùïú n g) :
    ContDiff ùïú n (fun x ‚Ü¶ (f x, r ‚Ä¢ f x + g x)) := by
  fun_prop

/- To summarise:
To prove differentiability goals, you can use
- (sometimes) use `simp` for goals `(f)deriv f x = y`
- use simp or argue by hand for goals `Has(F)DerivAt f f' x`
- use `fun_prop` for function properties like `DifferentiableAt` or `ContDiff`
-/

-- The implicit function theorem
#check ImplicitFunctionData.implicitFunction

-- The inverse function theorem
#check HasStrictFDerivAt.to_localInverse

/- If `f` is C¬π, its `fderiv` is continuous -/
#check ContDiff.continuous_fderiv

-- There is also a converse: if `f` is differentiable and has continuous fderiv, it is C¬π.
#check contDiff_one_iff_fderiv

-- This also holds for higher smoothness.
#check contDiff_succ_iff_fderiv


-- We can also take higher derivatives: these are denoted `iteratedFDeriv`
#check iteratedFDeriv

-- If `f : E ‚Üí F`, then `fderiv ùïú f x : E ‚ÜíL[ùïú] F` is a continuous linear map.
#check fderiv ùïú f

-- The `fderiv` of that is a continuous bilinear map, and so on:
variable {x‚ÇÄ : E}
#check iteratedFDeriv ùïú 2 f x‚ÇÄ

#check ContinuousMultilinearMap

-- The 0-th iterated fderiv is the function itself.
#check iteratedFDeriv_zero_apply

-- The first iterated fderiv is the fderiv.
-- Strictly speaking, their types are slightly different.
example (m : Fin 1 ‚Üí E) : iteratedFDeriv ùïú 1 f x‚ÇÄ m = (fderiv ùïú f x‚ÇÄ) (m 0) := by
  exact iteratedFDeriv_one_apply m

-- The k+1-th iterated fderiv is the fderiv of the k-th one.
#check iteratedFDeriv_succ_apply_left

end NormedSpace
